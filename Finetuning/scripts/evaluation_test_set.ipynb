{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGdy4uJq1kSQ"
      },
      "source": [
        "# Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HNJ04EH1qdW"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output \n",
        "!pip install tensorflow==2.9 t5 tensorflow-text==2.9\n",
        "#!pip install -q t5 tensorflow-text==2.4.3\n",
        "#!pip install -q tensorflow-text==2.8.0rc0\n",
        "#!pip install -U tensorflow-gcs-config==2.9.1\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g65Qu6MF1x2W",
        "outputId": "7db1f7be-acac-4d6c-8444-31083386c748"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing dependencies...\n"
          ]
        }
      ],
      "source": [
        "print(\"Installing dependencies...\")\n",
        "import functools\n",
        "import os\n",
        "import gin\n",
        "import tensorflow_gcs_config\n",
        "from google.colab import auth\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from contextlib import contextmanager\n",
        "import logging as py_logging\n",
        "import t5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2h1MRzBLtex2",
        "outputId": "ed80fc78-7ec8-4da3-93e3-2bdba032f4be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up GCS access...\n",
            "WARNING: auth.authenticate_user() will eventually stop supporting auth for Tensorflow on TPU devices. See auth.authenticate_service_account() instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on TPU: grpc://10.72.61.82:8470\n"
          ]
        }
      ],
      "source": [
        "TOKENIZER_DIR = \"bucket path\" #@param { type: \"string\" }\n",
        "if not TOKENIZER_DIR or TOKENIZER_DIR == \"gs://\": \n",
        "  raise ValueError(\"You must enter a TOKENIZER_DIR.\")\n",
        "\n",
        "print(\"Setting up GCS access...\")\n",
        "os.environ['USE_AUTH_EPHEM'] = '0'\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Set credentials for GCS reading/writing from Colab and TPU.\n",
        "TPU_TOPOLOGY = \"2x2\"\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  TPU_ADDRESS = tpu.get_master()\n",
        "  print('Running on TPU:', TPU_ADDRESS)\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "tf.config.experimental_connect_to_host(TPU_ADDRESS)\n",
        "tensorflow_gcs_config.configure_gcs_from_colab_auth()\n",
        "\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "\n",
        "#LOGGING\n",
        "tf.get_logger().propagate = False\n",
        "py_logging.root.setLevel('INFO')\n",
        "\n",
        "@contextmanager\n",
        "def tf_verbosity_level(level):\n",
        "  og_level = tf.logging.get_verbosity()\n",
        "  tf.logging.set_verbosity(level)\n",
        "  yield\n",
        "  tf.logging.set_verbosity(og_level)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0kxnD0T8EzI"
      },
      "source": [
        "# Load Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5h4PkSo_Kus",
        "outputId": "f201c1d7-ef3a-4c5e-fc6e-578b501c7735"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gs://bucket_context/eighth_experiment/code.model\n",
            "gs://bucket_context/eighth_experiment/code.vocab\n"
          ]
        }
      ],
      "source": [
        "vocab_model_path = 'gs://bucket_context/eighth_experiment/code.model'\n",
        "vocab_path = 'gs://bucket_context/eighth_experiment/code.vocab'\n",
        "print(vocab_model_path)\n",
        "print(vocab_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wq4YkN9_UkO"
      },
      "outputs": [],
      "source": [
        "from t5.data import postprocessors as t5_postprocessors\n",
        "from t5.seqio import Feature,SentencePieceVocabulary\n",
        "\n",
        "num_special_mask_tokens = 100 #@param {type: \"integer\"}\n",
        "\n",
        "def load_vocabulary():\n",
        "  return SentencePieceVocabulary(vocab_model_path, num_special_mask_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzX8GZjGVVne"
      },
      "outputs": [],
      "source": [
        "# change config file based on the finetuning context you want to perform\n",
        "config=\"call\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3yMW-___hYd"
      },
      "source": [
        "# Prepare Dataset for T5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glLJUm1dxIiH"
      },
      "outputs": [],
      "source": [
        "# save each dataset in a folder named ft_ followed by the current context\n",
        "train_path = 'gs://bucket_context/eighth_experiment/ft_{}/train.tsv'.format(config) #@param { type: \"string\" }\n",
        "eval_path = 'gs://bucket_context/eighth_experiment/ft_{}/eval.tsv'.format(config) #@param { type: \"string\" }\n",
        "test_path = 'gs://bucket_context/eighth_experiment/ft_{}/test.tsv'.format(config) #@param { type: \"string\" }\n",
        "finetune_datasets_paths = {\n",
        "    \"train\":      train_path,\n",
        "    \"validation\": test_path\n",
        "}\n",
        "\n",
        "# Useful when multi-task training \n",
        "# num_input_examples = dict(train=106382, validation=12020) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0NTLbyXvkCs"
      },
      "outputs": [],
      "source": [
        "def load_dataset(split, shuffle_files=True):\n",
        "  \"\"\"\n",
        "  Function to load .tsv dataset as a tf.data.Dataset in TensorFlow\n",
        "  \"\"\"\n",
        "  # We only have one file for each split.\n",
        "  del shuffle_files\n",
        "\n",
        "  # Load lines from the text file as examples.\n",
        "\n",
        "  ds = tf.data.TextLineDataset(finetune_datasets_paths[split])\n",
        "  ds = ds.map(functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n",
        "                          field_delim=\"\\t\", use_quote_delim=False)\n",
        "                          , \n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  \n",
        "  ds = ds.map(lambda *ex: dict(zip([\"input\", \"output\"], ex)))\n",
        "  return ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J4_UFsVmSPk"
      },
      "source": [
        "### A few examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__RNWuimAxS9",
        "outputId": "0375bf09-d9ce-4f0a-dc91-69f2db12e93d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A few raw validation examples...\n",
            "{'input': b\"void processError(String errorText) {<nl>this.statistics.incrementErrCount();<nl>this.lastError.set(errorText);<nl>this.connectError.set(errorText); // even if this isn't during connection, save it just in case<nl>// If we are connected && we get an authentication error, save it<nl>String url = this.getConnectedUrl();<nl>if (this.isConnected() && this.isAuthenticationError(errorText) && url != null) {<nl>this.serverAuthErrors.put(url, errorText);<nl>}<nl><extra_id_0><nl>try {<nl>this.callbackRunner.execute(() -> {<nl>try {<nl>options.getErrorListener().errorOccurred(this, errorText);<nl>} catch (Exception ex) {<nl>this.statistics.incrementExceptionCount();<nl>}<nl>});<nl>} catch (RejectedExecutionException re) {<nl>// Timing with shutdown, let it go<nl>}<nl>}<nl>} <sep> public void setTcpNoDelay(boolean on) {<nl>this.tcpNoDelay = on;<nl>if (this.session != null && this.session instanceof TCPSession) {<nl>try {<nl>((SocketChannel) ((TCPSession) this.session).getChannel()).socket().setTcpNoDelay(on);<nl>} catch (Exception ex) {<nl>logger.error(ex.getLocalizedMessage());<nl>}<nl>}<nl>}\", 'output': b'if (!this.callbackRunner.isShutdown()) {'}\n",
            "{'input': b\"void processError(String errorText) {<nl>this.statistics.incrementErrCount();<nl>this.lastError.set(errorText);<nl>this.connectError.set(errorText); // even if this isn't during connection, save it just in case<nl>// If we are connected && we get an authentication error, save it<nl>String url = this.getConnectedUrl();<nl>if (this.isConnected() && this.isAuthenticationError(errorText) && url != null) {<nl>this.serverAuthErrors.put(url, errorText);<nl>}<nl>if (!this.callbackRunner.isShutdown()) {<nl>try {<nl>this.callbackRunner.execute(() -> {<nl>try {<nl><extra_id_0><nl>} catch (Exception ex) {<nl>this.statistics.incrementExceptionCount();<nl>}<nl>});<nl>} catch (RejectedExecutionException re) {<nl>// Timing with shutdown, let it go<nl>}<nl>}<nl>} <sep> public void setTcpNoDelay(boolean on) {<nl>this.tcpNoDelay = on;<nl>if (this.session != null && this.session instanceof TCPSession) {<nl>try {<nl>((SocketChannel) ((TCPSession) this.session).getChannel()).socket().setTcpNoDelay(on);<nl>} catch (Exception ex) {<nl>logger.error(ex.getLocalizedMessage());<nl>}<nl>}<nl>}\", 'output': b'options.getErrorListener().errorOccurred(this, errorText);'}\n",
            "{'input': b'protected Map<String, NamedDataSchema> parseModels(DataList models) throws IOException<nl>{<nl><extra_id_0><nl>for (Object modelObj : models)<nl>{<nl>NamedDataSchema dataSchema;<nl>if (modelObj instanceof DataMap)<nl>{<nl>DataMap model = (DataMap)modelObj;<nl>dataSchema = (NamedDataSchema) RestSpecCodec.textToSchema(_dataCodec.mapToString(model), _dataSchemaResolver);<nl>}<nl>else if (modelObj instanceof String)<nl>{<nl>String str = (String)modelObj;<nl>dataSchema = (NamedDataSchema) RestSpecCodec.textToSchema(str, _dataSchemaResolver);<nl>}<nl>else<nl>{<nl>throw new IOException(\"Found \" + modelObj.getClass() + \" in models list; Models must be strings or DataMaps.\");<nl>}<nl>parsedModels.put(dataSchema.getFullName(), dataSchema);<nl>}<nl>return parsedModels;<nl>} <sep> public static Map<String, String> toStringMap(Map<String, Object> input) {<nl>Map<String, String> output = new HashMap<>(input.size());<nl>for (Map.Entry<String, Object> entry : input.entrySet()) {<nl>String key = entry.getKey();<nl>Object obj = entry.getValue();<nl>String str;<nl>if (obj instanceof redis.clients.jedis.GeoCoordinate) {<nl>redis.clients.jedis.GeoCoordinate geo = (redis.clients.jedis.GeoCoordinate) obj;<nl>str = geo.getLongitude() + \",\" + geo.getLatitude();<nl>} else if (obj instanceof String) {<nl>str = (String) obj;<nl>} else {<nl>str = obj.toString();<nl>}<nl>output.put(key, str);<nl>}<nl>return output;<nl>}', 'output': b'final Map<String, NamedDataSchema> parsedModels = new HashMap<>();'}\n",
            "{'input': b'@Override<nl><extra_id_0><nl>//Render only once to avoid overloading the server<nl>if (image == null) {<nl>return;<nl>}<nl>canvas.drawImage(0, 0, image);<nl>image = null;<nl>} <sep> @Override<nl>public String getAvatarUrl() {<nl>if (channelInfo.getLong(\"bio_image_id\") == 0) {<nl>return \"\";<nl>}<nl>return BandcampExtractorHelper.getImageUrl(channelInfo.getLong(\"bio_image_id\"), false);<nl>}', 'output': b'public void render(MapView v, final MapCanvas canvas, Player p) {'}\n",
            "{'input': b'@Override<nl>public void render(MapView v, final MapCanvas canvas, Player p) {<nl>//Render only once to avoid overloading the server<nl><extra_id_0><nl>}<nl>canvas.drawImage(0, 0, image);<nl>image = null;<nl>} <sep> public static void loadImage(final File file, final Renderer mapRenderer) {<nl>submitQuery(new WorkerRunnable<Void>() {<nl>@Override<nl>public Void run() throws Exception {<nl>BufferedImage image = ImageIO.read(file);<nl>mapRenderer.setImage(image);<nl>image.flush();//Safe to free<nl>return null;<nl>}<nl>});<nl>}', 'output': b'if (image == null) {<nl>return;'}\n"
          ]
        }
      ],
      "source": [
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(load_dataset(\"validation\").take(5)):\n",
        "  print(ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsbHi89ZA5-j"
      },
      "source": [
        "# Dataset Prepocessing "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bJZPQgjxKZ1"
      },
      "outputs": [],
      "source": [
        "from tensorflow_datasets.core.utils.type_utils import Shape\n",
        "\n",
        "def preprocessing(ds):\n",
        "  \"\"\"\n",
        "  Preprocess function to convert the tf.data.Dataset into a text-to-text format,\n",
        "  with both inputs and targets fields.\n",
        "  Param: tf.data.Dataset\n",
        "  Return: text-to-text format\n",
        "  \"\"\"\n",
        "  prefix = '' # no prefix for pretraining\n",
        "  def to_inputs_and_targets(ex):\n",
        "    x_input = tf.strings.strip(prefix + ex['input'])\n",
        "    y_label = tf.strings.strip(ex['output']) \n",
        "    inputs = tf.strings.join([x_input], separator=' ')\n",
        "    class_label = tf.strings.join([y_label], separator=' ')\n",
        "    return {'inputs': inputs, 'targets': class_label}\n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vymEYLbQCBRY"
      },
      "source": [
        "### A few examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOyas1ZqBVgE",
        "outputId": "c8a61da8-ca99-482e-a202-834862753756"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A few preprocessed train examples...\n",
            "{'inputs': b'<extra_id_0><nl>return MapHandlerRegistration.addHandler(this, MapEventType.SHADOW_CHANGED, handler,<nl>new ShadowChangeEventFormatter());<nl>} <sep> public final HandlerRegistration addAnimationChangeHandler(AnimationChangeMapHandler handler) {<nl>return MapHandlerRegistration.addHandler(this, MapEventType.ANIMATION_CHANGED, handler,<nl>new AnimationChangeEventFormatter());<nl>}', 'targets': b'public final HandlerRegistration addShadowChangeHandler(ShadowChangeMapHandler handler) {'}\n",
            "{'inputs': b'public static com.oracle.bmc.http.internal.WrappedInvocationBuilder fromRequest(<nl>com.oracle.bmc.http.internal.RestClient client,<nl>com.oracle.bmc.core.requests.UpdateVolumeGroupBackupRequest request) {<nl>Validate.notNull(request, \"request instance is required\");<nl>Validate.notBlank(<nl>request.getVolumeGroupBackupId(), \"volumeGroupBackupId must not be blank\");<nl>Validate.notNull(<nl>request.getUpdateVolumeGroupBackupDetails(),<nl>\"updateVolumeGroupBackupDetails is required\");<nl>com.oracle.bmc.http.internal.WrappedWebTarget target =<nl>client.getBaseTarget()<nl>.path(\"/20160918\")<nl>.path(\"volumeGroupBackups\")<nl>.path(<nl>com.oracle.bmc.util.internal.HttpUtils.encodePathSegment(<nl>request.getVolumeGroupBackupId()));<nl>com.oracle.bmc.http.internal.WrappedInvocationBuilder ib = target.request();<nl>ib.accept(javax.ws.rs.core.MediaType.APPLICATION_JSON);<nl>if (request.getIfMatch() != null) {<nl>ib.header(\"if-match\", request.getIfMatch());<nl>}<nl><extra_id_0><nl>}<nl>return ib;<nl>} <sep> public static com.oracle.bmc.http.internal.WrappedInvocationBuilder fromRequest(<nl>com.oracle.bmc.http.internal.RestClient client,<nl>com.oracle.bmc.core.requests.DeleteInternetGatewayRequest request) {<nl>Validate.notNull(request, \"request instance is required\");<nl>Validate.notBlank(request.getIgId(), \"igId must not be blank\");<nl>com.oracle.bmc.http.internal.WrappedWebTarget target =<nl>client.getBaseTarget()<nl>.path(\"/20160918\")<nl>.path(\"internetGateways\")<nl>.path(<nl>com.oracle.bmc.util.internal.HttpUtils.encodePathSegment(<nl>request.getIgId()));<nl>com.oracle.bmc.http.internal.WrappedInvocationBuilder ib = target.request();<nl>ib.accept(javax.ws.rs.core.MediaType.APPLICATION_JSON);<nl>if (request.getIfMatch() != null) {<nl>ib.header(\"if-match\", request.getIfMatch());<nl>}<nl>if (client.getClientConfigurator() != null) {<nl>client.getClientConfigurator().customizeRequest(request, ib);<nl>}<nl>return ib;<nl>}', 'targets': b'if (client.getClientConfigurator() != null) {<nl>client.getClientConfigurator().customizeRequest(request, ib);'}\n",
            "{'inputs': b'public static com.oracle.bmc.http.internal.WrappedInvocationBuilder fromRequest(<nl>com.oracle.bmc.http.internal.RestClient client,<nl>com.oracle.bmc.core.requests.UpdateVolumeGroupBackupRequest request) {<nl>Validate.notNull(request, \"request instance is required\");<nl>Validate.notBlank(<nl>request.getVolumeGroupBackupId(), \"volumeGroupBackupId must not be blank\");<nl>Validate.notNull(<nl>request.getUpdateVolumeGroupBackupDetails(),<nl>\"updateVolumeGroupBackupDetails is required\");<nl>com.oracle.bmc.http.internal.WrappedWebTarget target =<nl>client.getBaseTarget()<nl>.path(\"/20160918\")<nl>.path(\"volumeGroupBackups\")<nl>.path(<nl>com.oracle.bmc.util.internal.HttpUtils.encodePathSegment(<nl>request.getVolumeGroupBackupId()));<nl>com.oracle.bmc.http.internal.WrappedInvocationBuilder ib = target.request();<nl>ib.accept(javax.ws.rs.core.MediaType.APPLICATION_JSON);<nl>if (request.getIfMatch() != null) {<nl>ib.header(\"if-match\", request.getIfMatch());<nl>}<nl>if (client.getClientConfigurator() != null) {<nl>client.getClientConfigurator().customizeRequest(request, ib);<nl><extra_id_0><nl>return ib;<nl>} <sep> public static com.oracle.bmc.http.internal.WrappedInvocationBuilder fromRequest(<nl>com.oracle.bmc.http.internal.RestClient client,<nl>com.oracle.bmc.core.requests.UpdateDrgAttachmentRequest request) {<nl>Validate.notNull(request, \"request instance is required\");<nl>Validate.notBlank(request.getDrgAttachmentId(), \"drgAttachmentId must not be blank\");<nl>Validate.notNull(<nl>request.getUpdateDrgAttachmentDetails(), \"updateDrgAttachmentDetails is required\");<nl>com.oracle.bmc.http.internal.WrappedWebTarget target =<nl>client.getBaseTarget()<nl>.path(\"/20160918\")<nl>.path(\"drgAttachments\")<nl>.path(<nl>com.oracle.bmc.util.internal.HttpUtils.encodePathSegment(<nl>request.getDrgAttachmentId()));<nl>com.oracle.bmc.http.internal.WrappedInvocationBuilder ib = target.request();<nl>ib.accept(javax.ws.rs.core.MediaType.APPLICATION_JSON);<nl>if (request.getIfMatch() != null) {<nl>ib.header(\"if-match\", request.getIfMatch());<nl>}<nl>if (client.getClientConfigurator() != null) {<nl>client.getClientConfigurator().customizeRequest(request, ib);<nl>}<nl>return ib;<nl>}', 'targets': b'}'}\n",
            "{'inputs': b'<extra_id_0><nl>public OptionalInt findSingle() {<nl>if (iterator.hasNext()) {<nl>int singleCandidate = iterator.nextInt();<nl>if (iterator.hasNext()) {<nl>throw new IllegalStateException(\"IntStream contains more than one element\");<nl>} else {<nl>return OptionalInt.of(singleCandidate);<nl>}<nl>} else {<nl>return OptionalInt.empty();<nl>}<nl>} <sep> @NotNull<nl>public Optional<T> findSingle() {<nl>if (iterator.hasNext()) {<nl>T singleCandidate = iterator.next();<nl>if (iterator.hasNext()) {<nl>throw new IllegalStateException(\"Stream contains more than one element\");<nl>} else {<nl>return Optional.of(singleCandidate);<nl>}<nl>} else {<nl>return Optional.empty();<nl>}<nl>}', 'targets': b'@NotNull'}\n",
            "{'inputs': b'public byte[] getData() {<nl>if (packet != null) {<nl>Object dataSerializer = readObject(0, NMSUtils.packetDataSerializerClass);<nl>WrappedPacket byteBufWrapper = new WrappedPacket(new NMSPacket(dataSerializer));<nl>Object byteBuf = byteBufWrapper.readObject(0, NMSUtils.byteBufClass);<nl>return PacketEvents.get().getByteBufUtil().getBytes(byteBuf);<nl><extra_id_0><nl>return data;<nl>}<nl>} <sep> public byte[] getData() {<nl>if (packet != null) {<nl>switch (constructorMode) {<nl>case 0:<nl>return readByteArray(0);<nl>case 1:<nl>case 2:<nl>return PacketEvents.get().getByteBufUtil().getBytes(getBuffer());<nl>default:<nl>return new byte[0];<nl>}<nl>}<nl>return data;<nl>}', 'targets': b'} else {'}\n"
          ]
        }
      ],
      "source": [
        "print(\"A few preprocessed train examples...\")\n",
        "sample = tfds.as_numpy(preprocessing(load_dataset(\"train\").take(5)))\n",
        "for ex in sample:\n",
        "  print(ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OkQeLeh8Rst"
      },
      "source": [
        "# Creating Task and Mixture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PobLvzL18zzR",
        "outputId": "2ddd872a-f637-4c9e-e690-4db48470e24a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<seqio.dataset_providers.Mixture at 0x7f53cd2f6d90>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "DEFAULT_OUTPUT_FEATURES = {\n",
        "    \"inputs\": Feature(\n",
        "        vocabulary=load_vocabulary(), add_eos=True, required=False),\n",
        "    \"targets\": Feature(\n",
        "        vocabulary=load_vocabulary(), add_eos=True)\n",
        "    }\n",
        "\n",
        "TASK_NAME = \"ft\" #@param{ type : \"string\"}\n",
        "\n",
        "# TASK\n",
        "t5.data.TaskRegistry.remove(TASK_NAME)\n",
        "t5.data.TaskRegistry.add(\n",
        "    TASK_NAME,\n",
        "    # Function which returns a tf.data.Dataset\n",
        "    dataset_fn=load_dataset,\n",
        "    splits=[\"train\",\"validation\"],\n",
        "    # List of functions that preprocess the input tf.data.Dataset\n",
        "    text_preprocessor=[preprocessing],\n",
        "    # Accuracy is used as evaluation metric\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy],\n",
        "    # Not required, helps for mixing and auto-caching\n",
        "    # num_input_examples=num_input_examples,\n",
        "    output_features = DEFAULT_OUTPUT_FEATURES\n",
        ")\n",
        "\n",
        "MIXTURE_NAME = \"task\" #@param{ type : \"string\"}\n",
        "\n",
        "# MIXTURE\n",
        "t5.data.MixtureRegistry.remove(MIXTURE_NAME)\n",
        "t5.data.MixtureRegistry.add(\n",
        "    MIXTURE_NAME,\n",
        "    # List of tasks\n",
        "    [TASK_NAME],\n",
        "    default_rate=1.0\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwqLrVdGB6yy"
      },
      "source": [
        "### A few examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVHcIrjkA93r",
        "outputId": "ae4e0126-f076-46ac-ac83-a54a95825dfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Using an uncached FunctionDataset for training is not recommended since it often results in insufficient shuffling on restarts, resulting in overfitting. It is highly recommended that you cache this task before training with it or use a data source that supports lower-level shuffling (e.g., FileDataSource).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A few preprocessed training examples...\n",
            "{'inputs_pretokenized': b'protected Map<String, Map<String, Set<String>>> scan() {<nl>long start = System.currentTimeMillis();<nl>Map<String, Set<Map.Entry<String, String>>> collect = configuration.getScanners().stream().map(Scanner::index).distinct()<nl>.collect(Collectors.toMap(s -> s, s -> Collections.synchronizedSet(new HashSet<>())));<nl><extra_id_0><nl>Vfs.Dir dir = null;<nl>try {<nl>dir = Vfs.fromURL(url);<nl>for (Vfs.File file : dir.getFiles()) {<nl>if (doFilter(file, configuration.getInputsFilter())) {<nl>ClassFile classFile = null;<nl>for (Scanner scanner : configuration.getScanners()) {<nl>try {<nl>if (doFilter(file, scanner::acceptsInput)) {<nl>List<Map.Entry<String, String>> entries = scanner.scan(file);<nl>if (entries == null) {<nl>if (classFile == null) classFile = getClassFile(file);<nl>entries = scanner.scan(classFile);<nl>}<nl>if (entries != null) collect.get(scanner.index()).addAll(entries);<nl>}<nl>} catch (Exception e) {<nl>if (log != null) log.trace(\"could not scan file {} with scanner {}\", file.getRelativePath(), scanner.getClass().getSimpleName(), e);<nl>}<nl>}<nl>}<nl>}<nl>} catch (Exception e) {<nl>if (log != null) log.warn(\"could not create Vfs.Dir from url. ignoring the exception and continuing\", e);<nl>} finally {<nl>if (dir != null) dir.close();<nl>}<nl>});<nl>Map<String, Map<String, Set<String>>> storeMap =<nl>collect.entrySet().stream()<nl>.collect(Collectors.toMap(<nl>Map.Entry::getKey,<nl>entry -> entry.getValue().stream().filter(e -> e.getKey() != null)<nl>.collect(Collectors.groupingBy(<nl>Map.Entry::getKey,<nl>HashMap::new,<nl>Collectors.mapping(Map.Entry::getValue, Collectors.toSet())))));<nl>if (log != null) {<nl>int keys = 0, values = 0;<nl>for (Map<String, Set<String>> map : storeMap.values()) {<nl>keys += map.size();<nl>values += map.values().stream().mapToLong(Set::size).sum();<nl>}<nl>log.info(format(\"Reflections took %d ms to scan %d urls, producing %d keys and %d values\", System.currentTimeMillis() - start, urls.size(), keys, values));<nl>}<nl>return storeMap;<nl>} <sep> private void formUnionEnums()<nl>{<nl>sharedNameToField.values().forEach(field -><nl>{<nl>if (field.isEnum())<nl>{<nl>final List<Value> fieldValues = field.values();<nl>// Rename collisions by name<nl>final Map<String, Map<String, Long>> nameToReprToCount = fieldValues.stream().collect(<nl>groupingBy(Value::description,<nl>groupingBy(Value::representation, Collectors.counting())));<nl>// Make a new unique Value for every input with the name collisions fixed<nl>final List<Value> values = nameToReprToCount<nl>.entrySet().stream().flatMap(e -><nl>{<nl>final String name = e.getKey();<nl>final Map<String, Long> reprToCount = e.getValue();<nl>final String commonRepr = findCommonName(reprToCount);<nl>final Stream<Value> commonValues =<nl>LongStream.range(0, reprToCount.get(commonRepr)).mapToObj(i -><nl>new Value(commonRepr, name));<nl>final HashSet<String> otherReprs = new HashSet<>(reprToCount.keySet());<nl>otherReprs.remove(commonRepr);<nl>final Stream<Value> otherValues = otherReprs.stream().flatMap(repr -><nl>{<nl>final String newName = name + \"_\" + DictionaryParser.enumDescriptionToJavaName(repr);<nl>return LongStream.range(0, reprToCount.get(repr))<nl>.mapToObj(i -> new Value(repr, newName));<nl>});<nl>return concat(commonValues, otherValues);<nl>})<nl>.collect(toList());<nl>// Add name collisions by representation to Javadoc<nl>final Map<String, Map<String, Long>> reprToNameToCount = values.stream().collect(<nl>groupingBy(Value::representation,<nl>groupingBy(Value::description, Collectors.counting())));<nl>final List<Value> finalValues = reprToNameToCount<nl>.entrySet().stream().map(e -><nl>{<nl>final String repr = e.getKey();<nl>final Map<String, Long> nameToCount = e.getValue();<nl>final String commonName = findCommonName(nameToCount);<nl>final Value value = new Value(repr, commonName);<nl>if (nameToCount.size() > 1)<nl>{<nl>final Set<String> otherNames = nameToCount.keySet();<nl>otherNames.remove(commonName);<nl>value.alternativeNames(new ArrayList<>(otherNames));<nl>}<nl>return value;<nl>})<nl>.collect(toList());<nl>fieldValues.clear();<nl>fieldValues.addAll(finalValues);<nl>}<nl>});<nl>}', 'inputs': array([ 132,  114, 2029, ...,    7, 9366,    1], dtype=int32), 'targets_pretokenized': b'Set<URL> urls = configuration.getUrls();<nl>(configuration.isParallel() ? urls.stream().parallel() : urls.stream()).forEach(url -> {', 'targets': array([  249,  2029,   801,  1047,  4168,    15,   370,    35,   438,\n",
            "        4801,  2388,     7,   325,  5979,    35,  1022,  9477,   842,\n",
            "          65,  4168,    35,  2444,  2715, 26695,   842,    45,  4168,\n",
            "          35,  2444,  5799,    35, 31006,   325,  2594,   217,    13,\n",
            "           1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'public byte[] recordAPICall(final serverObjects post, final String servletName, final String type, final String comment, int time, String unit) {<nl>if (post.containsKey(TABLE_API_COL_APICALL_PK)) {<nl>// this api call has already been stored somewhere.<nl>return recordAPICall(post, servletName, type, comment);<nl>}<nl>if (time < 0 || unit == null || unit.isEmpty() || \"minutes,hours,days\".indexOf(unit) < 0) {<nl>time = 0; unit = \"\";<nl>} else {<nl>if (unit.equals(\"minutes\") && time < 10) time = 10;<nl>}<nl>final String transactionToken = post.get(TransactionManager.TRANSACTION_TOKEN_PARAM);<nl>if(transactionToken != null) {<nl>post.put(TransactionManager.TRANSACTION_TOKEN_PARAM, \"\");<nl>}<nl>// generate the apicall url - without the apicall attributes<nl>final String apiurl = /*\"http://localhost:\" + getConfig(\"port\", \"8090\") +*/ \"/\" + servletName + \"?\" + post.toString();<nl>if(transactionToken != null) {<nl>post.put(TransactionManager.TRANSACTION_TOKEN_PARAM, transactionToken);<nl><extra_id_0><nl>}<nl>byte[] pk = null;<nl>// insert entry<nl>try {<nl>// create and insert new entry<nl>Data data = new Data();<nl>data.put(TABLE_API_COL_TYPE, UTF8.getBytes(type));<nl>data.put(TABLE_API_COL_COMMENT, UTF8.getBytes(comment));<nl>byte[] date = ASCII.getBytes(GenericFormatter.SHORT_MILSEC_FORMATTER.format());<nl>data.put(TABLE_API_COL_DATE_RECORDING, date);<nl>data.put(TABLE_API_COL_DATE_LAST_EXEC, date);<nl>data.put(TABLE_API_COL_URL, UTF8.getBytes(apiurl));<nl>// insert APICALL attributes<nl>data.put(TABLE_API_COL_APICALL_COUNT, UTF8.getBytes(\"1\"));<nl>data.put(TABLE_API_COL_APICALL_SCHEDULE_TIME, ASCII.getBytes(Integer.toString(time)));<nl>data.put(TABLE_API_COL_APICALL_SCHEDULE_UNIT, UTF8.getBytes(unit));<nl>calculateAPIScheduler(data, false); // set next execution time<nl>pk = super.insert(TABLE_API_NAME, data);<nl>} catch (final IOException e) {<nl>ConcurrentLog.logException(e);<nl>} catch (final SpaceExceededException e) {<nl>ConcurrentLog.logException(e);<nl>}<nl>ConcurrentLog.info(\"APICALL\", apiurl);<nl>return pk;<nl>} <sep> public byte[] recordAPICall(final serverObjects post, final String servletName, final String type, final String comment) {<nl>// remove the apicall attributes from the post object<nl>String[] pks = post.remove(TABLE_API_COL_APICALL_PK);<nl>byte[] pk = pks == null ? null : UTF8.getBytes(pks[0]);<nl>// generate the apicall url - without the apicall attributes<nl>final String apiurl = generateRecordedURL(post, servletName);<nl>// read old entry from the apicall table (if exists)<nl>Row row = null;<nl>try {<nl>row = (pk == null) ? null : super.select(TABLE_API_NAME, pk);<nl>} catch (final IOException e) {<nl>ConcurrentLog.logException(e);<nl>} catch (final SpaceExceededException e) {<nl>ConcurrentLog.logException(e);<nl>}<nl>// insert or update entry<nl>try {<nl>if (row == null) {<nl>// create and insert new entry<nl>Data data = new Data();<nl>data.put(TABLE_API_COL_TYPE, UTF8.getBytes(type));<nl>data.put(TABLE_API_COL_COMMENT, UTF8.getBytes(comment));<nl>byte[] date = UTF8.getBytes(GenericFormatter.SHORT_MILSEC_FORMATTER.format());<nl>data.put(TABLE_API_COL_DATE_RECORDING, date);<nl>data.put(TABLE_API_COL_DATE_LAST_EXEC, date);<nl>data.put(TABLE_API_COL_URL, UTF8.getBytes(apiurl));<nl>// insert APICALL attributes<nl>data.put(TABLE_API_COL_APICALL_COUNT, \"1\");<nl>pk = super.insert(TABLE_API_NAME, data);<nl>} else {<nl>// modify and update existing entry<nl>// modify date attributes and patch old values<nl>row.put(TABLE_API_COL_DATE_LAST_EXEC, UTF8.getBytes(GenericFormatter.SHORT_MILSEC_FORMATTER.format()));<nl>if (!row.containsKey(TABLE_API_COL_DATE_RECORDING)) row.put(TABLE_API_COL_DATE_RECORDING, row.get(TABLE_API_COL_DATE));<nl>row.remove(TABLE_API_COL_DATE);<nl>// insert APICALL attributes<nl>row.put(TABLE_API_COL_APICALL_COUNT, row.get(TABLE_API_COL_APICALL_COUNT, 1) + 1);<nl>calculateAPIScheduler(row, false);<nl>super.update(TABLE_API_NAME, row);<nl>assert pk != null;<nl>}<nl>} catch (final IOException e) {<nl>ConcurrentLog.logException(e);<nl>} catch (final SpaceExceededException e) {<nl>ConcurrentLog.logException(e);<nl>}<nl>ConcurrentLog.info(\"APICALL\", apiurl);<nl>return pk;<nl>}', 'inputs': array([  29,  145, 5035, ...,   18, 3695,    1], dtype=int32), 'targets_pretokenized': b'} else {<nl>post.remove(TransactionManager.TRANSACTION_TOKEN_PARAM);', 'targets': array([   14,    57,    13,     7,  7745,    35,  4218,   325,   817,\n",
            "         229,    35, 12862,    18,  3146,    18,  3497,   908,     1],\n",
            "      dtype=int32)}\n",
            "{'inputs_pretokenized': b'public byte[] recordAPICall(final serverObjects post, final String servletName, final String type, final String comment, int time, String unit) {<nl>if (post.containsKey(TABLE_API_COL_APICALL_PK)) {<nl>// this api call has already been stored somewhere.<nl>return recordAPICall(post, servletName, type, comment);<nl>}<nl>if (time < 0 || unit == null || unit.isEmpty() || \"minutes,hours,days\".indexOf(unit) < 0) {<nl>time = 0; unit = \"\";<nl>} else {<nl>if (unit.equals(\"minutes\") && time < 10) time = 10;<nl>}<nl>final String transactionToken = post.get(TransactionManager.TRANSACTION_TOKEN_PARAM);<nl>if(transactionToken != null) {<nl><extra_id_0><nl>// generate the apicall url - without the apicall attributes<nl>final String apiurl = /*\"http://localhost:\" + getConfig(\"port\", \"8090\") +*/ \"/\" + servletName + \"?\" + post.toString();<nl>if(transactionToken != null) {<nl>post.put(TransactionManager.TRANSACTION_TOKEN_PARAM, transactionToken);<nl>} else {<nl>post.remove(TransactionManager.TRANSACTION_TOKEN_PARAM);<nl>}<nl>byte[] pk = null;<nl>// insert entry<nl>try {<nl>// create and insert new entry<nl>Data data = new Data();<nl>data.put(TABLE_API_COL_TYPE, UTF8.getBytes(type));<nl>data.put(TABLE_API_COL_COMMENT, UTF8.getBytes(comment));<nl>byte[] date = ASCII.getBytes(GenericFormatter.SHORT_MILSEC_FORMATTER.format());<nl>data.put(TABLE_API_COL_DATE_RECORDING, date);<nl>data.put(TABLE_API_COL_DATE_LAST_EXEC, date);<nl>data.put(TABLE_API_COL_URL, UTF8.getBytes(apiurl));<nl>// insert APICALL attributes<nl>data.put(TABLE_API_COL_APICALL_COUNT, UTF8.getBytes(\"1\"));<nl>data.put(TABLE_API_COL_APICALL_SCHEDULE_TIME, ASCII.getBytes(Integer.toString(time)));<nl>data.put(TABLE_API_COL_APICALL_SCHEDULE_UNIT, UTF8.getBytes(unit));<nl>calculateAPIScheduler(data, false); // set next execution time<nl>pk = super.insert(TABLE_API_NAME, data);<nl>} catch (final IOException e) {<nl>ConcurrentLog.logException(e);<nl>} catch (final SpaceExceededException e) {<nl>ConcurrentLog.logException(e);<nl>}<nl>ConcurrentLog.info(\"APICALL\", apiurl);<nl>return pk;<nl>} <sep> public byte[] recordAPICall(final serverObjects post, final String servletName, final String type, final String comment) {<nl>// remove the apicall attributes from the post object<nl>String[] pks = post.remove(TABLE_API_COL_APICALL_PK);<nl>byte[] pk = pks == null ? null : UTF8.getBytes(pks[0]);<nl>// generate the apicall url - without the apicall attributes<nl>final String apiurl = generateRecordedURL(post, servletName);<nl>// read old entry from the apicall table (if exists)<nl>Row row = null;<nl>try {<nl>row = (pk == null) ? null : super.select(TABLE_API_NAME, pk);<nl>} catch (final IOException e) {<nl>ConcurrentLog.logException(e);<nl>} catch (final SpaceExceededException e) {<nl>ConcurrentLog.logException(e);<nl>}<nl>// insert or update entry<nl>try {<nl>if (row == null) {<nl>// create and insert new entry<nl>Data data = new Data();<nl>data.put(TABLE_API_COL_TYPE, UTF8.getBytes(type));<nl>data.put(TABLE_API_COL_COMMENT, UTF8.getBytes(comment));<nl>byte[] date = UTF8.getBytes(GenericFormatter.SHORT_MILSEC_FORMATTER.format());<nl>data.put(TABLE_API_COL_DATE_RECORDING, date);<nl>data.put(TABLE_API_COL_DATE_LAST_EXEC, date);<nl>data.put(TABLE_API_COL_URL, UTF8.getBytes(apiurl));<nl>// insert APICALL attributes<nl>data.put(TABLE_API_COL_APICALL_COUNT, \"1\");<nl>pk = super.insert(TABLE_API_NAME, data);<nl>} else {<nl>// modify and update existing entry<nl>// modify date attributes and patch old values<nl>row.put(TABLE_API_COL_DATE_LAST_EXEC, UTF8.getBytes(GenericFormatter.SHORT_MILSEC_FORMATTER.format()));<nl>if (!row.containsKey(TABLE_API_COL_DATE_RECORDING)) row.put(TABLE_API_COL_DATE_RECORDING, row.get(TABLE_API_COL_DATE));<nl>row.remove(TABLE_API_COL_DATE);<nl>// insert APICALL attributes<nl>row.put(TABLE_API_COL_APICALL_COUNT, row.get(TABLE_API_COL_APICALL_COUNT, 1) + 1);<nl>calculateAPIScheduler(row, false);<nl>super.update(TABLE_API_NAME, row);<nl>assert pk != null;<nl>}<nl>} catch (final IOException e) {<nl>ConcurrentLog.logException(e);<nl>} catch (final SpaceExceededException e) {<nl>ConcurrentLog.logException(e);<nl>}<nl>ConcurrentLog.info(\"APICALL\", apiurl);<nl>return pk;<nl>}', 'inputs': array([  29,  145, 5035, ...,   18, 3695,    1], dtype=int32), 'targets_pretokenized': b'post.put(TransactionManager.TRANSACTION_TOKEN_PARAM, \"\");<nl>}', 'targets': array([  971,    35,  5059,   325,   817,   229,    35, 12862,    18,\n",
            "        3146,    18,  3497,   101,    22,  3918,     7,  2977,     1],\n",
            "      dtype=int32)}\n",
            "{'inputs_pretokenized': b'private void processObjectNode(<nl>JsonNode node,<nl>Supplier<ImmutableJsonShreddedRow.Builder> rowBuilder,<nl>List<JsonShreddedRow> result) {<nl>// empty object, simply create a reference to empty node and return<nl>if (node.isEmpty()) {<nl>ImmutableJsonShreddedRow row =<nl>rowBuilder.get().stringValue(DocsApiConstants.EMPTY_OBJECT_MARKER).build();<nl>result.add(row);<nl>return;<nl>}<nl>node.fields()<nl>.forEachRemaining(<nl>field -> {<nl>String fieldName = field.getKey();<nl><extra_id_0><nl>\"JSON objects containing empty field names are not supported at the moment.\";<nl>throw new ErrorCodeRuntimeException(<nl>ErrorCode.DOCS_API_GENERAL_INVALID_FIELD_NAME, msg);<nl>}<nl>// check for valid field name<nl>if (DocsApiUtils.containsIllegalSequences(fieldName)) {<nl>String msg =<nl>String.format(<nl>\"Array paths contained in square brackets, periods, single quotes, and backslash are not allowed in field names, invalid field %s\",<nl>fieldName);<nl>throw new ErrorCodeRuntimeException(<nl>ErrorCode.DOCS_API_GENERAL_INVALID_FIELD_NAME, msg);<nl>}<nl>// escape the field path<nl>// then create new next row builder<nl>String fieldPath = DocsApiUtils.convertEscapedCharacters(fieldName);<nl>Supplier<ImmutableJsonShreddedRow.Builder> nextRowBuilder =<nl>() -> rowBuilder.get().addPath(fieldPath);<nl>// process inner node and increase the index<nl>processNode(field.getValue(), nextRowBuilder, result);<nl>});<nl>} <sep> public List<PluginPackageAttributeDto> entityView(String packageName, String entityName) {<nl>PluginPackageDataModel latestDataModelEntity = tryFetchLatestAvailableDataModelEntity(packageName);<nl>if (latestDataModelEntity == null) {<nl>String msg = String.format(\"Cannot find data model by package name: [%s] and entity name: [%s]\",<nl>packageName, entityName);<nl>log.error(msg);<nl>throw new WecubeCoreException(\"3302\", msg, packageName, entityName);<nl>}<nl>List<PluginPackageAttributeDto> result = new ArrayList<>();<nl>List<PluginPackageEntities> foundEntityList = pluginPackageEntitiesMapper<nl>.selectAllByPackageNameAndEntityNameAndDataModelVersion(packageName, entityName,<nl>latestDataModelEntity.getVersion());<nl>if (foundEntityList == null || foundEntityList.isEmpty()) {<nl>log.warn(\"empty entity list for {} {} {}\", packageName, entityName, latestDataModelEntity.getVersion());<nl>return result;<nl>}<nl>PluginPackageEntities foundEntity = foundEntityList.get(0);<nl>List<PluginPackageAttributes> pluginPackageAttributes = pluginPackageAttributesMapper<nl>.selectAllByEntity(foundEntity.getId());<nl>if (pluginPackageAttributes == null || pluginPackageAttributes.isEmpty()) {<nl>log.info(\"empty attributes for {}\", foundEntity.getId());<nl>return result;<nl>}<nl>for (PluginPackageAttributes a : pluginPackageAttributes) {<nl>PluginPackageAttributeDto dto = buildPluginPackageAttributeDto(foundEntity, a);<nl>result.add(dto);<nl>}<nl>return result;<nl>}', 'inputs': array([   66,    48,   309,   200,   161,   325,     7,   896,   161,\n",
            "         137,   101,     7,  1630,  2029,  5473,   896,   203, 11446,\n",
            "         201,   153,   201,   688,    35,   177,  1047,   403,   177,\n",
            "         101,     7,   108,  2029,   896,   203, 11446,   201,   153,\n",
            "         201,   688,  1047,    69,   349,    13,     7,  8428,   506,\n",
            "         149,   101,  6748,    92,    49,   702,    37,   506,   137,\n",
            "         100,    17,     7,  1484,     8,  2589,    35, 20477,  5799,\n",
            "          13,     7,  5473,   896,   203, 11446,   201,   153,   201,\n",
            "         688,   403,    15,     7,  4480,   177,    35,   438,  2715,\n",
            "        3135,   120,   325,  6888,  1108,   655,    35,  9401,    18,\n",
            "        1501,    18, 10337,  2275,  4707,  2388,     7,  4337,    35,\n",
            "        1751,   325,  4480,   908,     7,  2930,   785,     7,  2977,\n",
            "           7,  2589,    35,  9089,   842,     7,    35, 23850,   325,\n",
            "           7,  3004,   217,    13,     7,   196,  1025,    15,   181,\n",
            "          35,   438,   133,  2388,     7,  2029, 22479,    18,   421,\n",
            "        1678,  1047,     7,    30,  1518,  1094,  1324,   506,   181,\n",
            "         905,   231,    97,  1231,   441,    25, 11863,   242,   785,\n",
            "           7,  7040,    23,  4998,   414,   325,     7,  5990,    35,\n",
            "       12549,   203,    18,  2325,    18, 18076,    18,  5684,    18,\n",
            "        2371,    18,   356,   101,   387,   908,     7,  2977,     7,\n",
            "        8428,   254,    44,   794,   181,    61,     7,  1484,     8,\n",
            "        6888,  1108,   347,    35, 10817,  6019, 12053,   325,  3004,\n",
            "          77,  5445,    13,     7,   196,   387,    15,     7,   196,\n",
            "          35,  3882,   325,     7,    30,   373,  2232,  3492,    71,\n",
            "        8580, 20712,   101,  2021,    28,   101,   956, 10032,   101,\n",
            "         100, 20498,   231,    97,  1526,    71,   181,   905,   101,\n",
            "        1433,   181,   313,    28,  4597,     7,  3004,    77,   908,\n",
            "           7,  7040,    23,  4998,   414,   325,     7,  5990,    35,\n",
            "       12549,   203,    18,  2325,    18, 18076,    18,  5684,    18,\n",
            "        2371,    18,   356,   101,   387,   908,     7,  2977,     7,\n",
            "        8428,  2186,    25,   181,   134,     7,  8428,   623,    92,\n",
            "          23,   176,   403,   191,     7,   196, 13421,    15,    24,\n",
            "        6888,  1108,   347,    35, 12197,  9022,  6770,   325,  3004,\n",
            "          77,   908,     7,  1630,  2029,  5473,   896,   203, 11446,\n",
            "         201,   153,   201,   688,    35,   177,  1047,   176,   688,\n",
            "         177,    15,     7,   842,   217,   403,   177,    35,   438,\n",
            "        2715,  1751,   182,   325,  3004,   182,   908,     7,  8428,\n",
            "         309,   983,   137,   100,  5628,    25,   128,     7,  4359,\n",
            "         161,   325,  3004,    35, 17505,  5555,   176,   688,   177,\n",
            "         101,    69,   908,     7,  2977,   908,     7,  2977,    19,\n",
            "          28,  8100,  1047,    29,    67,  2029,  1332,   936,   380,\n",
            "        1707,  1047,   381,   404,   325,   196,  2338,   101,    26,\n",
            "        9376,   349,    13,     7,  1332,   936, 11695,  3782, 11695,\n",
            "         478,    15,    72,  3856,  8631,  2250, 11695,   478,   325,\n",
            "        5354,    77,   908,     7,  1484,     8, 23984, 11695,   478,\n",
            "          41,    27,   349,    13,     7,   196,   387,    15,    26,\n",
            "          35,  3882,  1084,  1210,   288,   116,   341,   225,  1397,\n",
            "          61,    94,  3622,    28,  1151,   100,   381,    61,    94,\n",
            "        3622,    28,  2276,   101,     7,  5354,    77,   101,  9376,\n",
            "         908,     7,  1316,    35,  1983,   325,  6134,   908,     7,\n",
            "        7040,    23,  1293,   584, 17860,  3048,    42,  1084,  5626,\n",
            "        3435,  4597,   387,   101,  2338,   101,  9376,   908,     7,\n",
            "        2977,     7,   108,  2029,  1332,   936,   380,  1707,  1047,\n",
            "          69,    15,    23,   199, 26121,  2388,     7,   108,  2029,\n",
            "        1332,   936,  2743,  1047,   353,   478,   108,    15,  1269,\n",
            "         936,  2743,  1093,     7,    35,  3675,   769,   532,  6423,\n",
            "         423, 12056,   423, 11695,   410,   325,  5354,    77,   101,\n",
            "        9376,   101,     7, 23984, 11695,   478,    35,   438,   410,\n",
            "        3731,     7,  1484,     8,  9918,   478,   108,    41,    27,\n",
            "         118,   353,   478,   108,    35, 20477,  5799,    13,     7,\n",
            "        1316,    35, 19639,  1084,  5312,   381,   127,    44,   918,\n",
            "         918,   697,   101,  2338,   101,  9376,   101,  3782, 11695,\n",
            "         478,    35,   438,   410,  3731,     7,  2930,    69,   785,\n",
            "           7,  2977,     7,  1332,   936,  2743,   353,   478,    15,\n",
            "         353,   478,   108,    35,   438, 20603,   785,     7,   108,\n",
            "        2029,  1332,   936,   633,  1047,  1269,   936,   633,    15,\n",
            "        1269,   936,   633,  1093,     7,    35,  3675,   769,   532,\n",
            "         478,   325,  9918,   478,    35,   438,    93,  3731,     7,\n",
            "        1484,     8, 10920,   936,   633,    41,    27,   118,  1269,\n",
            "         936,   633,    35, 20477,  5799,    13,     7,  1316,    35,\n",
            "        2474,  1084,  5312,   624,    44,   697,   101,   353,   478,\n",
            "          35,   438,    93,  3731,     7,  2930,    69,   785,     7,\n",
            "        2977,     7,  3470,     8,  1332,   936,   633,    49,    45,\n",
            "        1269,   936,   633,   349,    13,     7,  1332,   936,   380,\n",
            "        1707,  3233,    15,   205,  1332,   936,   380,  1707,   325,\n",
            "        9918,   478,   101,    49,   908,     7,  4337,    35,  1751,\n",
            "         325,   153,  1205,   908,     7,  2977,     7,  2930,    69,\n",
            "         785,     7,  2977,     1], dtype=int32), 'targets_pretokenized': b'if (fieldName.isEmpty()) {<nl>String msg =', 'targets': array([   16,     8,  3004,    77,    35, 20477,  5799,    13,     7,\n",
            "         196,   387,    15,     1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'@Override<nl>public Iterable<Code> expand(ValueSetInfo valueSet) throws ResourceNotFoundException {<nl>if (resolveByUrl(valueSet) == null) {<nl>return Collections.emptyList();<nl>}<nl>Parameters respParam = fhirClient<nl>.operation()<nl>.onInstance(new IdType(\"ValueSet\", valueSet.getId()))<nl>.named(\"expand\")<nl>.withNoParameters(Parameters.class)<nl><extra_id_0><nl>.execute();<nl>ValueSet expanded = (ValueSet) respParam.getParameter().get(0).getResource();<nl>List<Code> codes = new ArrayList<>();<nl>for (ValueSet.ValueSetExpansionContainsComponent codeInfo : expanded.getExpansion().getContains()) {<nl>Code nextCode = new Code()<nl>.withCode(codeInfo.getCode())<nl>.withSystem(codeInfo.getSystem())<nl>.withVersion(codeInfo.getVersion())<nl>.withDisplay(codeInfo.getDisplay());<nl>codes.add(nextCode);<nl>}<nl>return codes;<nl>} <sep> @Override<nl>public Code lookup(Code code, CodeSystemInfo codeSystem) throws ResourceNotFoundException {<nl>Parameters respParam = fhirClient<nl>.operation()<nl>.onType(CodeSystem.class)<nl>.named(\"lookup\")<nl>.withParameter(Parameters.class, \"code\", new CodeType(code.getCode()))<nl>.andParameter(\"system\", new UriType(codeSystem.getId()))<nl>.execute();<nl>StringType display = (StringType) respParam.getParameter(\"display\");<nl>if( display != null ) {<nl>code.withDisplay( display.getValue() );<nl>}<nl>return code.withSystem(codeSystem.getId());<nl>}', 'inputs': array([   20,  2913,     7,  4943,   844,  2029,   492,  1047,  3219,\n",
            "         325, 11768,   150,    55,   239,   349,    56,    24, 11773,\n",
            "          42,    13,     7,  1484,     8, 18097,   532,   417,   325,\n",
            "         790,   239,   349,    41,    27,   349,    13,     7,  2930,\n",
            "         622,    35,  5312,   108,  2388,     7,  2977,     7,   524,\n",
            "        1257,   689,    15,  9260,   317,     7,    35,  9060,   842,\n",
            "           7,    35,  1312,   386,   325,  2401,  2798,    64,  1084,\n",
            "       11768,  4597,    55,   239,    35,   438,    93, 28609,     7,\n",
            "          35,   437,   153,  1084, 20840,    30,   349,     7,    35,\n",
            "        5396,   673,   524,   325,   524,    35,  1270,   349,     7,\n",
            "        2029, 22479,    18,   421,  1678,  1047,     7,    35,  9787,\n",
            "        2388,     7, 11768,  5705,    15,     8, 11768,   349,  1257,\n",
            "         689,    35,   438,   501,  2715,   438, 20603,    35,   438,\n",
            "         278,  2388,     7,   108,  2029,   492,  1047,   104,    28,\n",
            "          15,    23,   199, 26121,  2388,     7,  3470,     8, 11768,\n",
            "          35, 11768, 10487,  4874,   534,   104,   150,    45,  5705,\n",
            "          35,   438, 10487,  2715,   438,  4874,  5799,    13,     7,\n",
            "         492,   176,   492,    15,    23,  2542,   842,     7,    35,\n",
            "        5396,   492,   325,   261,   150,    35,   438,   492,  5799,\n",
            "           7,    35,  5396,  1139,   325,   261,   150,    35,   438,\n",
            "        1139,  5799,     7,    35,  5396,   410,   325,   261,   150,\n",
            "          35,   438,   410,  5799,     7,    35,  5396,  3226,   325,\n",
            "         261,   150,    35,   438,  3226,  3731,     7,   261,    28,\n",
            "          35,  1751,   325,  4032,   492,   908,     7,  2977,     7,\n",
            "        2930,   104,    28,   785,     7,  2977,    19,    28,  8100,\n",
            "        1047,    20,  2913,     7,  4943,  2542,  1314,   325,   492,\n",
            "         104,   101,    24, 13034,   150,   104,  1139,   349,    56,\n",
            "          24, 11773,    42,    13,     7,   524,  1257,   689,    15,\n",
            "        9260,   317,     7,    35,  9060,   842,     7,    35,  1312,\n",
            "          64,   325, 13034,    35,  1270,   349,     7,    35,   437,\n",
            "         153,  1084, 20697,    30,   349,     7,    35,  5396,   501,\n",
            "         325,   524,    35,  1270,   101,    22,   261,  4597,    23,\n",
            "        2542,    64,   325,   261,    35,   438,   492, 28609,     7,\n",
            "          35,  2873,   501,  1084,  5574,  4597,    23,  4178,    64,\n",
            "         325,   261,  1139,    35,   438,    93, 28609,     7,    35,\n",
            "        9787,  2388,     7,   196,    64,  1628,    15,     8,   196,\n",
            "          64,   349,  1257,   689,    35,   438,   501,  1084,  8599,\n",
            "        3918,     7,  1484,   325,  1628,    53,    27,     9,    13,\n",
            "           7,   261,    35,  5396,  3226,   325,  1628,    35, 17505,\n",
            "         842,     9,   785,     7,  2977,     7,  2930,   104,    35,\n",
            "        5396,  1139,   325,   261,  1139,    35,   438,    93,  3731,\n",
            "           7,  2977,     1], dtype=int32), 'targets_pretokenized': b'.useHttpGet()', 'targets': array([  10, 4009, 1818,  924,  842,    1], dtype=int32)}\n"
          ]
        }
      ],
      "source": [
        "finetuning_task = t5.data.TaskRegistry.get(TASK_NAME)\n",
        "ds = finetuning_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 1024, \"targets\": 1024})\n",
        "print(\"A few preprocessed training examples...\")\n",
        "for ex in tfds.as_numpy(ds.take(5)):\n",
        "  print(ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQobwjpVCJbu"
      },
      "source": [
        "# Creating Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcNX1djT-ugc"
      },
      "outputs": [],
      "source": [
        "from t5 import models\n",
        "\n",
        "FLAGS = tf.app.flags.FLAGS\n",
        "tf.app.flags.DEFINE_string ('f', '', 'kernel')\n",
        "\n",
        "#See https://github.com/google-research/text-to-text-transfer-transformer if you want to scale up the model\n",
        "MODEL_SIZE = \"base\"  \n",
        "\n",
        "MODEL_DIR = 'gs://bucket_context/eighth_experiment/ft_final_model_{}'.format(config)\n",
        "\n",
        "PRETRAINED_DIR='gs://bucket_context/eighth_experiment/pt_model'\n",
        "\n",
        "\n",
        "model_parallelism, train_batch_size, keep_checkpoint_max = {\n",
        "    \"small\": (1, 64, 50),\n",
        "    \"base\": (2, 32, 100),\n",
        "    \"large\": (8, 64, 4),\n",
        "    \"3B\": (8, 16, 1),\n",
        "    \"11B\": (8, 16, 1)}[MODEL_SIZE]\n",
        "\n",
        "\n",
        "tf.io.gfile.makedirs(MODEL_DIR)\n",
        "\n",
        "model = t5.models.MtfModel(\n",
        "    model_dir=MODEL_DIR,\n",
        "    tpu=TPU_ADDRESS,\n",
        "    tpu_topology=TPU_TOPOLOGY,\n",
        "    model_parallelism=model_parallelism,\n",
        "    batch_size=train_batch_size,\n",
        "    sequence_length={\"inputs\": 1024, \"targets\": 1024},\n",
        "    learning_rate_schedule = 0.001,\n",
        "    save_checkpoints_steps=5000,\n",
        "    keep_checkpoint_max=keep_checkpoint_max\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "id": "11qeh_bnEfgr",
        "outputId": "d9975784-7bf0-4f49-af06-604461d30c9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:system_path_file_exists:gs://bucket_context/eighth_experiment/ft_final_model_most_similar_crystalbleu/operative_config.gin\n",
            "ERROR:root:Path not found: gs://bucket_context/eighth_experiment/ft_final_model_most_similar_crystalbleu/operative_config.gin\n",
            "INFO:absl:Adding task 'ft' with predict metric_fn(s).\n",
            "INFO:absl:Skipping packing/padding for 'ft' since sequence length is None.\n",
            "INFO:absl:Setting sequence lengths to {'inputs': 1345, 'targets': 179}\n",
            "INFO:absl:Evaluating checkpoint step: 1670000\n",
            "INFO:absl:Padding 'ft' with sequence lengths: {'inputs': 1345, 'targets': 179}\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-701717352854>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Use a larger batch size for evaluation, which requires less memory.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m model.eval(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmixture_or_task_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"task\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# mixture_or_task_name=\"all_tasks\",\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/t5/models/mtf_model.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, mixture_or_task_name, checkpoint_steps, summary_dir, split, eval_with_score, compute_sequence_length)\u001b[0m\n\u001b[1;32m    342\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0meval_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m     utils.run_eval(\n\u001b[0m\u001b[1;32m    345\u001b[0m         \u001b[0mmixture_or_task_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmixture_or_task_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         predict_or_score_fn=functools.partial(self._predict_or_score_fn,\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/t5/models/utils.py\u001b[0m in \u001b[0;36mrun_eval\u001b[0;34m(mixture_or_task_name, predict_or_score_fn, checkpoint_steps, dataset_fn, summary_dir, split, sequence_length, batch_size)\u001b[0m\n\u001b[1;32m    332\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcheckpoint_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluating checkpoint step: %d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m     outputs = predict_or_score_fn(\n\u001b[0m\u001b[1;32m    335\u001b[0m         \u001b[0mcheckpoint_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0mvocabulary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/t5/models/mtf_model.py\u001b[0m in \u001b[0;36m_predict_or_score_fn\u001b[0;34m(self, tasks, vocabulary, checkpoint_step, sequence_length, split, eval_with_score, **unused_kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m           vocabulary)\n\u001b[1;32m    293\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m       outputs = [\n\u001b[0m\u001b[1;32m    295\u001b[0m           tf.compat.as_text(d) for d in mtf_utils.decode(\n\u001b[1;32m    296\u001b[0m               estimator, estimator_input_fn, vocabulary, checkpoint_path)\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/t5/models/mtf_model.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    292\u001b[0m           vocabulary)\n\u001b[1;32m    293\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m       outputs = [\n\u001b[0m\u001b[1;32m    295\u001b[0m           tf.compat.as_text(d) for d in mtf_utils.decode(\n\u001b[1;32m    296\u001b[0m               estimator, estimator_input_fn, vocabulary, checkpoint_path)\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/mesh_tensorflow/transformer/utils.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(estimator, input_fn, vocabulary, checkpoint_path)\u001b[0m\n\u001b[1;32m   1346\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m     input_string = _maybe_detokenize(\n\u001b[1;32m   1350\u001b[0m         result[\"inputs\"], inputs_vocabulary(vocabulary))\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, input_fn, predict_keys, hooks, checkpoint_path, yield_single_examples)\u001b[0m\n\u001b[1;32m   3132\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rendezvous\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_fn_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendezvous\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3133\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3134\u001b[0;31m       for result in super(TPUEstimator, self).predict(\n\u001b[0m\u001b[1;32m   3135\u001b[0m           \u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3136\u001b[0m           \u001b[0mpredict_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredict_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, input_fn, predict_keys, hooks, checkpoint_path, yield_single_examples)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0mall_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0mall_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m         with tf.compat.v1.train.MonitoredSession(\n\u001b[0m\u001b[1;32m    635\u001b[0m             session_creator=tf.compat.v1.train.ChiefSessionCreator(\n\u001b[1;32m    636\u001b[0m                 \u001b[0mcheckpoint_filename_with_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session_creator, hooks, stop_grace_period_secs)\u001b[0m\n\u001b[1;32m   1052\u001b[0m                \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m                stop_grace_period_secs=120):\n\u001b[0;32m-> 1054\u001b[0;31m     super(MonitoredSession, self).__init__(\n\u001b[0m\u001b[1;32m   1055\u001b[0m         \u001b[0msession_creator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m         \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session_creator, hooks, should_recover, stop_grace_period_secs)\u001b[0m\n\u001b[1;32m    755\u001b[0m         stop_grace_period_secs=stop_grace_period_secs)\n\u001b[1;32m    756\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshould_recover\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_RecoverableSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinated_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinated_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sess_creator)\u001b[0m\n\u001b[1;32m   1261\u001b[0m     \"\"\"\n\u001b[1;32m   1262\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess_creator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess_creator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m     \u001b[0m_WrappedSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36m_create_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1266\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1268\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1269\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m         logging.info(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mcreate_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    908\u001b[0m       \u001b[0;34m\"\"\"Creates a coordinated session.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;31m# Keep the tf_sess for unit testing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_sess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m       \u001b[0;31m# We don't want coordinator to suppress any exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCoordinator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_stop_exception_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mcreate_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcreate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scaffold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m     return self._get_session_manager().prepare_session(\n\u001b[0m\u001b[1;32m    670\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_master\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0msaver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scaffold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py\u001b[0m in \u001b[0;36mprepare_session\u001b[0;34m(self, master, init_op, saver, checkpoint_dir, checkpoint_filename_with_path, wait_for_checkpoint, max_wait_secs, config, init_feed_dict, init_fn)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \"\"\"\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m     sess, is_loaded_from_checkpoint = self._restore_checkpoint(\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0mmaster\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0msaver\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py\u001b[0m in \u001b[0;36m_restore_checkpoint\u001b[0;34m(self, master, saver, checkpoint_dir, checkpoint_filename_with_path, wait_for_checkpoint, max_wait_secs, config)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcheckpoint_filename_with_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m       _restore_checkpoint_and_maybe_run_saved_model_initializers(\n\u001b[0m\u001b[1;32m    230\u001b[0m           sess, saver, checkpoint_filename_with_path)\n\u001b[1;32m    231\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py\u001b[0m in \u001b[0;36m_restore_checkpoint_and_maybe_run_saved_model_initializers\u001b[0;34m(sess, saver, path)\u001b[0m\n\u001b[1;32m     65\u001b[0m   \u001b[0;31m# init will restore the variables from the SavedModel variables directory.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;31m# Initializing/restoring twice is not ideal but there's no other way to do it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m   \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1415\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1416\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1417\u001b[0;31m         sess.run(self.saver_def.restore_op_name,\n\u001b[0m\u001b[1;32m   1418\u001b[0m                  {self.saver_def.filename_tensor_name: save_path})\n\u001b[1;32m   1419\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[1;32m    968\u001b[0m                          run_metadata_ptr)\n\u001b[1;32m    969\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[0;31m# or if the call is a partial run that specifies feeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[1;32m   1191\u001b[0m                              feed_dict_tensor, options, run_metadata)\n\u001b[1;32m   1192\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[1;32m   1371\u001b[0m                            run_metadata)\n\u001b[1;32m   1372\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1375\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1377\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1378\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0m\u001b[1;32m   1361\u001b[0m                                       target_list, run_metadata)\n\u001b[1;32m   1362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1451\u001b[0m   def _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list,\n\u001b[1;32m   1452\u001b[0m                           run_metadata):\n\u001b[0;32m-> 1453\u001b[0;31m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0m\u001b[1;32m   1454\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1455\u001b[0m                                             run_metadata)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Use a larger batch size for evaluation, which requires less memory.\n",
        "model.batch_size = 512\n",
        "model.eval(\n",
        "    mixture_or_task_name=\"task\",\n",
        "    # mixture_or_task_name=\"all_tasks\",\n",
        "    checkpoint_steps=-1 #evaluate only last checkpoint\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdK__RW_RWQn"
      },
      "outputs": [],
      "source": [
        "# set the best checkpoint for the current context\n",
        "checkpoint=1615000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47aUJoKPDxfC",
        "outputId": "7a3ee418-5851-43f5-f25d-6ff6cd7d07be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:system_path_file_exists:gs://bucket_context/eighth_experiment/ft_final_model_most_similar_crystalbleu/operative_config.gin\n",
            "ERROR:root:Path not found: gs://bucket_context/eighth_experiment/ft_final_model_most_similar_crystalbleu/operative_config.gin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1615000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "From /usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:825: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n"
          ]
        }
      ],
      "source": [
        "vocabulary_predict=load_vocabulary()\n",
        "\n",
        "model.batch_size = 128\n",
        "\n",
        "print(checkpoint)\n",
        "model.predict(input_file=\"gs://bucket_context/eighth_experiment/testset/{}/inputs.txt\".format(config), \n",
        "              output_file=\"gs://bucket_context/eighth_experiment/result_testset/predictions_{}.txt\".format(config),\n",
        "              checkpoint_steps=checkpoint, beam_size=1, temperature=0.0, keep_top_k=-1, vocabulary=vocabulary_predict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_Qs1dzBV9h5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}